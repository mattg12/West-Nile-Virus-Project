{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting West Nile Virus\n",
    "\n",
    "## Matt Garton, Katrina Miller, Rex Littlefield\n",
    "\n",
    "## General Assembly Boston - DSI - September 2018\n",
    "\n",
    "Link to presentation: https://docs.google.com/presentation/d/1MMZhOJnfq-JJzOlV3dmPOhjXRPjlVWUu07YNDUGYcJU/edit#slide=id.p\n",
    "\n",
    "# Model Building Notebook\n",
    "\n",
    "Goal: Build a classification model to predict where and when West Nile Virus will be found in Chicago.\n",
    "\n",
    "The purpose of this notebook is to build, run, and evaluate classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier, AdaBoostClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, precision_recall_fscore_support, classification_report, roc_auc_score, roc_curve, auc, confusion_matrix\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [] # Instantiate an empty list, which will hold dictionaries of model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_auc(fpr, tpr, roc_auc):\n",
    "    '''Plots the ROC-AUC for a given model and test data'''\n",
    "\n",
    "    # Create a plot of the ROC-AUC Curve\n",
    "    plt.figure(figsize = (10,6))\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = {}'.format(round(roc_auc, 2)))\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.xlabel(\"False Positive Rate (1 - Specificity)\")\n",
    "    plt.ylabel(\"True Positive Rate (Sensitivity)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_results(model, model_name, X_train, X_test, y_train, y_test):\n",
    "    '''For a classification model, predict y values and calculate Accuracy, return a confustion matrix,\n",
    "    calculate TP, TN, FP, FN, and other metrics on both training and testing data'''\n",
    "    \n",
    "    # Training Scores\n",
    "    y_predict_tr = model.predict(X_train)\n",
    "    accuracy_train = model.score(X_train, y_train)\n",
    "    \n",
    "    # Testing Scores\n",
    "    y_predict = model.predict(X_test)\n",
    "    accuracy_test = model.score(X_test, y_test)\n",
    "    precision, recall, fbeta_score, support = precision_recall_fscore_support(y_test, y_predict)\n",
    "    \n",
    "    # ROC-AUC\n",
    "    probas = model.predict_proba(X_test)\n",
    "    preds = probas[:,1]\n",
    "    fpr, tpr, threshold = roc_curve(y_test, model.predict_proba(X_test)[:,1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Create dictionary to store model results\n",
    "    model_dict = {\n",
    "        'Model': model_name,\n",
    "        'Train Accuracy': accuracy_train,\n",
    "        'Test Accuracy': accuracy_test,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F_1 Score': fbeta_score,\n",
    "        'Support': support,\n",
    "        'ROC-AUC': roc_auc\n",
    "    }\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_predict) \n",
    "\n",
    "    print(classification_report(y_test, y_predict), '\\n')\n",
    "    print('ROC-AUC: {}'.format(roc_auc))\n",
    "    \n",
    "    plot_roc_auc(fpr, tpr, roc_auc)\n",
    "    \n",
    "    return model_dict, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "west_nile = pd.read_csv('../data/preprocessed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up X and y\n",
    "features = ['carrier', 'Latitude', 'Longitude', 'NumMosquitos', 'Tavg', 'PrecipTotal', 'StnPressure',\n",
    "           'BR', 'HZ', 'RA', 'TSRA', 'VCTS', 'DZ', 'TS', 'FG']\n",
    "\n",
    "X = west_nile[features]\n",
    "y = west_nile['WnvPresent']\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, stratify = y)\n",
    "\n",
    "# Oversample minority class in y\n",
    "smote = SMOTE()\n",
    "X_train, y_train = smote.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling: First Attempts\n",
    "\n",
    "For the first models, we are using some of the features that early EDA suggested would be useful predictors, including the 'carrier' species, latitude and longitude, number of mosquitos, temperatrue, precipitation, station pressure, and weather codes.\n",
    "\n",
    "Two models are tested: K-Nearest Neighbors, Decision Tree, and Logistic Regression. In the KNN and Regression Models, we grid search over degrees of polynomial features (as well as the 'interaction only' argument. KNN hyperparameters are also tuned.\n",
    "\n",
    "After initial attempts on the unbalanced dataset 'as is' turned out very unsusccessful - ROC-AUC scores not meaningfully higher than 0.50 - we implemented an oversampling technique from the imblearn module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neighbors Model\n",
    "\n",
    "# Set up pipeline\n",
    "steps = [\n",
    "    ('poly', PolynomialFeatures()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "]\n",
    "\n",
    "pipe = Pipeline(steps)\n",
    "\n",
    "# Set up gridsearch\n",
    "grid_params = {\n",
    "    'poly__degree': [2],\n",
    "    'poly__interaction_only': [True, False],\n",
    "    'knn__n_neighbors': [3],\n",
    "    'knn__p':[2]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, grid_params, scoring = 'roc_auc', n_jobs = -1)\n",
    "\n",
    "# Fit model\n",
    "model = gs.fit(X_train, y_train)\n",
    "\n",
    "# Run and evaluate model\n",
    "model_dict, cm = get_classification_results(model, 'K-Nearest Neighbors (basic)',\n",
    "                                            X_train, X_test,\n",
    "                                            y_train, y_test)\n",
    "\n",
    "models.append(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Model\n",
    "\n",
    "# Set up pipeline\n",
    "steps = [\n",
    "    ('poly', PolynomialFeatures()),\n",
    "    ('lr', LogisticRegression())\n",
    "]\n",
    "\n",
    "pipe = Pipeline(steps)\n",
    "\n",
    "# Set up gridsearch\n",
    "grid_params = {\n",
    "    'poly__degree':[1,2,3],\n",
    "    'poly__interaction_only':[True, False]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, grid_params, scoring = 'roc_auc', n_jobs = -1)\n",
    "model = gs.fit(X_train, y_train)\n",
    "\n",
    "# Run and evaluate model\n",
    "model_dict, cm = get_classification_results(model, 'Logistic Regression (basic)',\n",
    "                                            X_train, X_test,\n",
    "                                            y_train, y_test)\n",
    "\n",
    "models.append(model_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling: PCA Approach\n",
    "\n",
    "In order to capture all of the information in the 'weather' data while accounting for multicollinearity and filtering out noise, we decided to try applying principal component analysis to the weather data (quantitative columns only), and then adding the mose useful principal components back into the model.\n",
    "\n",
    "As above, a K-Nearest Neighbors model and Logistic Regression Model are tested, along with a Decision Tree Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up X and y\n",
    "features = ['carrier', 'Latitude', 'Longitude', 'NumMosquitos','Tmax', \n",
    "            'Tmin', 'Tavg', 'Depart', 'DewPoint', 'WetBulb', 'Heat','Cool',\n",
    "            'PrecipTotal', 'StnPressure', 'SeaLevel', 'ResultSpeed', 'ResultDir', \n",
    "            'AvgSpeed','BR', 'HZ', 'RA', 'TSRA', 'VCTS', 'DZ', 'TS', 'FG']\n",
    "\n",
    "X = west_nile[features]\n",
    "y = west_nile['WnvPresent']\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, stratify = y)\n",
    "\n",
    "# Oversample minority class in y\n",
    "smote = SMOTE()\n",
    "X_train, y_train = smote.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the next steps to work, need to turn X_train back into a df\n",
    "X_train = pd.DataFrame(X_train, columns = features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# set up matrix of weather variables to perform PCA\n",
    "weather_vectors = ['Tmax', 'Tmin', 'Tavg', 'Depart', 'DewPoint', 'WetBulb', 'Heat','Cool',\n",
    "                  'PrecipTotal', 'StnPressure', 'SeaLevel', 'ResultSpeed', 'ResultDir', \n",
    "                  'AvgSpeed']\n",
    "\n",
    "X_w_train = X_train[weather_vectors]\n",
    "X_w_test = X_test[weather_vectors]\n",
    "\n",
    "# perform PCA - fit/transform\n",
    "pca = PCA()\n",
    "pca = pca.fit(X_w_train)\n",
    "\n",
    "Z_train = pca.transform(X_w_train)\n",
    "Z_test = pca.transform(X_w_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_exp = pca.explained_variance_ratio_\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "print('Cumulative explained variance: ', cum_var_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit to Matt Brems for the code to plot PCA Variance Explained.\n",
    "# Taken from Solution code to PCA Lecture\n",
    "\n",
    "plt.figure(figsize=(9,7))\n",
    "\n",
    "# Plot the explained variance\n",
    "component_number = range(len(var_exp))\n",
    "plt.plot(component_number, var_exp, lw=3)\n",
    "\n",
    "# Add horizontal lines at y=0 and y=100\n",
    "plt.axhline(y=0, linewidth=1, color='grey', ls='dashed')\n",
    "plt.axhline(y=1, linewidth=1, color='grey', ls='dashed')\n",
    "\n",
    "# Set the x and y axis limits\n",
    "ax = plt.gca()\n",
    "ax.set_xlim([-1,26])\n",
    "ax.set_ylim([-0.05,1.05])\n",
    "\n",
    "# Label the axes\n",
    "ax.set_ylabel('variance explained', fontsize=16)\n",
    "ax.set_xlabel('component', fontsize=16)\n",
    "\n",
    "# Make the tick labels bigger\n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(12) \n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(12) \n",
    "    \n",
    "# Add title\n",
    "ax.set_title('Component vs Variance Explained\\n', fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Results\n",
    "\n",
    "The top three principal components explain ~92% of the variation in the weather data. Those will be incorporated into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top three principal components\n",
    "pc_train = Z_train[:,0:3]\n",
    "pc_train = pd.DataFrame(pc_train, columns = ['PC1','PC2','PC3'])\n",
    "\n",
    "pc_test = Z_test[:,0:3]\n",
    "pc_test = pd.DataFrame(pc_test, columns = ['PC1','PC2','PC3'])\n",
    "\n",
    "# Include PCs into X_train and X_test\n",
    "X_train.drop(columns = weather_vectors, inplace = True)\n",
    "X_train.reset_index(drop = True, inplace = True)\n",
    "X_train = pd.concat([X_train, pc_train], axis = 1)\n",
    "\n",
    "X_test.drop(columns = weather_vectors, inplace = True)\n",
    "X_test.reset_index(drop = True, inplace = True)\n",
    "X_test = pd.concat([X_test, pc_test], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neighbors Model\n",
    "\n",
    "# Set up pipeline\n",
    "steps = [\n",
    "    ('poly', PolynomialFeatures()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "]\n",
    "\n",
    "pipe = Pipeline(steps)\n",
    "\n",
    "# Set up gridsearch\n",
    "grid_params = {\n",
    "    'poly__degree': [3],\n",
    "    'poly__interaction_only': [False],\n",
    "    'knn__n_neighbors': [3],\n",
    "    'knn__p':[2]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, grid_params, scoring = 'roc_auc', n_jobs = -1)\n",
    "\n",
    "# Fit model\n",
    "model = gs.fit(X_train, y_train)\n",
    "\n",
    "# Run and evaluate model\n",
    "model_dict, cm = get_classification_results(gs, 'K-Nearest Neighbors (pca)',\n",
    "                                            X_train, X_test,\n",
    "                                            y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.append(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Decision Tree\n",
    "\n",
    "# Set up gridsearch\n",
    "grid_params = {\n",
    "    'max_depth':[None, 1, 3, 5 ,7],\n",
    "    'max_features':[None, 1, 2, 3, 4, 5]\n",
    "}\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "gs = GridSearchCV(dt, grid_params, scoring = 'roc_auc', n_jobs = -1)\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "# Run and evaluate model\n",
    "model_dict, cm = get_classification_results(gs, 'Decision Tree (pca)',\n",
    "                                            X_train, X_test,\n",
    "                                            y_train, y_test)\n",
    "\n",
    "models.append(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Model\n",
    "\n",
    "# Set up pipeline\n",
    "steps = [\n",
    "    ('poly', PolynomialFeatures()),\n",
    "    ('lr', LogisticRegression())\n",
    "]\n",
    "\n",
    "pipe = Pipeline(steps)\n",
    "\n",
    "# Set up gridsearch\n",
    "grid_params = {\n",
    "    'poly__degree':[1,2,3],\n",
    "    'poly__interaction_only':[True, False]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, grid_params, scoring = 'roc_auc', n_jobs = -1)\n",
    "model = gs.fit(X_train, y_train)\n",
    "\n",
    "# Run and evaluate model\n",
    "model_dict, cm = get_classification_results(model, 'Logistic Regression (pca)',\n",
    "                                            X_train, X_test,\n",
    "                                            y_train, y_test)\n",
    "\n",
    "models.append(model_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling: Apply Outside Research\n",
    "\n",
    "The following models attempt to incorporate what we have learned from outside research. A high temp dummy, a dry conditions dummy, and dummies to indicate that the observation occurs during one of the migration seasons for the American Robin (a known spreader of West Nile virus) are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up X and y\n",
    "features = ['carrier', 'Latitude', 'Longitude', 'NumMosquitos', 'high_temp', 'PrecipTotal', 'fall_migrate', 'spring_migrate', 'StnPressure',\n",
    "           'BR', 'HZ', 'RA', 'TSRA', 'VCTS', 'DZ', 'TS', 'FG']\n",
    "\n",
    "X = west_nile[features]\n",
    "y = west_nile['WnvPresent']\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, stratify = y)\n",
    "\n",
    "# Oversample minority class in y\n",
    "smote = SMOTE()\n",
    "X_train, y_train = smote.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up pipeline\n",
    "steps = [\n",
    "    ('poly', PolynomialFeatures()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "]\n",
    "\n",
    "pipe = Pipeline(steps)\n",
    "\n",
    "# Set up gridsearch\n",
    "grid_params = {\n",
    "    'poly__degree': [2],\n",
    "    'poly__interaction_only': [True, False],\n",
    "    'knn__n_neighbors': [3],\n",
    "    'knn__p':[2]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, grid_params, scoring = 'roc_auc', n_jobs = -1)\n",
    "\n",
    "# Fit model\n",
    "model = gs.fit(X_train, y_train)\n",
    "\n",
    "# Run and evaluate model\n",
    "model_dict, cm = get_classification_results(gs, 'K-Nearest Neighbors (research)',\n",
    "                                            X_train, X_test,\n",
    "                                            y_train, y_test)\n",
    "\n",
    "models.append(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Decision Tree\n",
    "\n",
    "grid_params = {\n",
    "    'max_depth':[None, 1, 3, 5 ,7],\n",
    "    'max_features':[None, 1, 2, 3, 4, 5]\n",
    "}\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "gs = GridSearchCV(dt, grid_params, scoring = 'roc_auc', n_jobs = -1)\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "# Run and evaluate model\n",
    "model_dict, cm = get_classification_results(gs, 'Decision Tree (research)',\n",
    "                                            X_train, X_test,\n",
    "                                            y_train, y_test)\n",
    "\n",
    "models.append(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Model\n",
    "\n",
    "# Set up pipeline\n",
    "steps = [\n",
    "    ('poly', PolynomialFeatures()),\n",
    "    ('lr', LogisticRegression())\n",
    "]\n",
    "\n",
    "pipe = Pipeline(steps)\n",
    "\n",
    "# Set up gridsearch\n",
    "grid_params = {\n",
    "    'poly__degree':[1, 2, 3],\n",
    "    'poly__interaction_only':[True, False]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, grid_params, scoring = 'roc_auc', n_jobs = -1)\n",
    "model = gs.fit(X_train, y_train)\n",
    "\n",
    "# Run and evaluate model\n",
    "model_dict, cm = get_classification_results(model, 'Logistic Regression (research)',\n",
    "                                            X_train, X_test,\n",
    "                                            y_train, y_test)\n",
    "\n",
    "models.append(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe to view the confusion matrix\n",
    "cm_df = pd.DataFrame(cm, columns = ['Predicted No West Nile', 'Predicted West Nile'],\n",
    "                    index = ['Actual No West Nile', 'Actual West Nile'])\n",
    "\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model\n",
    "\n",
    "The Logistic Regression model using the engineered features based on outside research, with polynomial terms up to degree three, allowing for squared/cubed terms, ended up being the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Attempt: Combine outside research and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Set up X and y\n",
    "features = ['carrier', 'Latitude', 'Longitude', 'NumMosquitos','Tmax', \n",
    "            'Tmin', 'Tavg', 'Depart', 'DewPoint', 'WetBulb', 'Heat','Cool',\n",
    "            'PrecipTotal', 'StnPressure', 'SeaLevel', 'ResultSpeed', 'ResultDir', \n",
    "            'AvgSpeed','high_temp', 'fall_migrate', 'spring_migrate','BR', 'HZ', \n",
    "            'RA', 'TSRA', 'VCTS', 'DZ', 'TS', 'FG']\n",
    "\n",
    "X = west_nile[features]\n",
    "y = west_nile['WnvPresent']\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, stratify = y)\n",
    "\n",
    "# Oversample minority class in y\n",
    "smote = SMOTE()\n",
    "X_train, y_train = smote.fit_sample(X_train, y_train)\n",
    "\n",
    "# for PCA to work, need to turn X_train back into a df\n",
    "X_train = pd.DataFrame(X_train, columns = features)\n",
    "\n",
    "# set up matrix of weather variables to perform PCA\n",
    "weather_vectors = ['Tmax', 'Tmin', 'Tavg', 'Depart', 'DewPoint', 'WetBulb', 'Heat','Cool',\n",
    "                  'PrecipTotal', 'StnPressure', 'SeaLevel', 'ResultSpeed', 'ResultDir', \n",
    "                  'AvgSpeed']\n",
    "\n",
    "X_w_train = X_train[weather_vectors]\n",
    "X_w_test = X_test[weather_vectors]\n",
    "\n",
    "\n",
    "# Perform PCA - fit/transform\n",
    "pca = PCA()\n",
    "pca = pca.fit(X_w_train)\n",
    "\n",
    "Z_train = pca.transform(X_w_train)\n",
    "Z_test = pca.transform(X_w_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_exp = pca.explained_variance_ratio_\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "print('Cumulative explained variance: ', cum_var_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top three principal components\n",
    "pc_train = Z_train[:,0:3]\n",
    "pc_train = pd.DataFrame(pc_train, columns = ['PC1','PC2','PC3'])\n",
    "\n",
    "pc_test = Z_test[:,0:3]\n",
    "pc_test = pd.DataFrame(pc_test, columns = ['PC1','PC2','PC3'])\n",
    "\n",
    "# Include PCs into X_train and X_test\n",
    "X_train.drop(columns = weather_vectors, inplace = True)\n",
    "X_train.reset_index(drop = True, inplace = True)\n",
    "X_train = pd.concat([X_train, pc_train], axis = 1)\n",
    "\n",
    "X_test.drop(columns = weather_vectors, inplace = True)\n",
    "X_test.reset_index(drop = True, inplace = True)\n",
    "X_test = pd.concat([X_test, pc_test], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Model\n",
    "\n",
    "# Set up pipeline\n",
    "steps = [\n",
    "    ('poly', PolynomialFeatures()),\n",
    "    ('lr', LogisticRegression())\n",
    "]\n",
    "\n",
    "pipe = Pipeline(steps)\n",
    "\n",
    "# Set up gridsearch\n",
    "grid_params = {\n",
    "    'poly__degree':[1,2,3],\n",
    "    'poly__interaction_only':[True, False]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, grid_params, scoring = 'roc_auc', n_jobs = -1)\n",
    "model = gs.fit(X_train, y_train)\n",
    "\n",
    "# Run and evaluate model\n",
    "model_dict, cm = get_classification_results(model, 'Logistic Regression (research + pca)',\n",
    "                                            X_train, X_test,\n",
    "                                            y_train, y_test)\n",
    "\n",
    "models.append(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe to view the confusion matrix\n",
    "cm_df = pd.DataFrame(cm, columns = ['Predicted No West Nile', 'Predicted West Nile'],\n",
    "                    index = ['Actual No West Nile', 'Actual West Nile'])\n",
    "\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Model','Train Accuracy','Test Accuracy','Precision','Recall','Support','F_1 Score','ROC-AUC']]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling: Ensemble Methods\n",
    "\n",
    "Looking at the results above, it appears that each of the models suffers from overfitting, particularly (and unsurprisingly) the Decision Tree model. Therefore, I would like to try some ensemble methods to see if it is possible to improve on the results obtained thus far. I'll try this for each feature matrix created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_models = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Run the below cell for the 'Basic' feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up X and y\n",
    "features = ['carrier', 'Latitude', 'Longitude', 'NumMosquitos', 'Tavg', 'PrecipTotal', 'StnPressure',\n",
    "           'BR', 'HZ', 'RA', 'TSRA', 'VCTS', 'DZ', 'TS', 'FG']\n",
    "\n",
    "X = west_nile[features]\n",
    "y = west_nile['WnvPresent']\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, stratify = y)\n",
    "\n",
    "# Oversample minority class in y\n",
    "smote = SMOTE()\n",
    "X_train, y_train = smote.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Run the below cell for the 'PCA' feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up X and y\n",
    "features = ['carrier', 'Latitude', 'Longitude', 'NumMosquitos','Tmax', \n",
    "            'Tmin', 'Tavg', 'Depart', 'DewPoint', 'WetBulb', 'Heat','Cool',\n",
    "            'PrecipTotal', 'StnPressure', 'SeaLevel', 'ResultSpeed', 'ResultDir', \n",
    "            'AvgSpeed','BR', 'HZ', 'RA', 'TSRA', 'VCTS', 'DZ', 'TS', 'FG']\n",
    "\n",
    "X = west_nile[features]\n",
    "y = west_nile['WnvPresent']\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, stratify = y)\n",
    "\n",
    "# Oversample minority class in y\n",
    "smote = SMOTE()\n",
    "X_train, y_train = smote.fit_sample(X_train, y_train)\n",
    "\n",
    "# for the next steps to work, need to turn X_train back into a df\n",
    "X_train = pd.DataFrame(X_train, columns = features)\n",
    "\n",
    "# set up matrix of weather variables to perform PCA\n",
    "weather_vectors = ['Tmax', 'Tmin', 'Tavg', 'Depart', 'DewPoint', 'WetBulb', 'Heat','Cool',\n",
    "                  'PrecipTotal', 'StnPressure', 'SeaLevel', 'ResultSpeed', 'ResultDir', \n",
    "                  'AvgSpeed']\n",
    "\n",
    "X_w_train = X_train[weather_vectors]\n",
    "X_w_test = X_test[weather_vectors]\n",
    "\n",
    "# Perform PCA - fit/transform\n",
    "pca = PCA()\n",
    "pca = pca.fit(X_w_train)\n",
    "\n",
    "Z_train = pca.transform(X_w_train)\n",
    "Z_test = pca.transform(X_w_test)\n",
    "\n",
    "# Select top three principal components\n",
    "pc_train = Z_train[:,0:3]\n",
    "pc_train = pd.DataFrame(pc_train, columns = ['PC1','PC2','PC3'])\n",
    "\n",
    "pc_test = Z_test[:,0:3]\n",
    "pc_test = pd.DataFrame(pc_test, columns = ['PC1','PC2','PC3'])\n",
    "\n",
    "# Include PCs into X_train and X_test\n",
    "X_train.drop(columns = weather_vectors, inplace = True)\n",
    "X_train.reset_index(drop = True, inplace = True)\n",
    "X_train = pd.concat([X_train, pc_train], axis = 1)\n",
    "\n",
    "X_test.drop(columns = weather_vectors, inplace = True)\n",
    "X_test.reset_index(drop = True, inplace = True)\n",
    "X_test = pd.concat([X_test, pc_test], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Run the below cell to use the 'Research-Based' feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up X and y\n",
    "features = ['carrier', 'Latitude', 'Longitude', 'NumMosquitos', 'high_temp', 'PrecipTotal', 'fall_migrate', 'spring_migrate', 'StnPressure',\n",
    "           'BR', 'HZ', 'RA', 'TSRA', 'VCTS', 'DZ', 'TS', 'FG']\n",
    "\n",
    "X = west_nile[features]\n",
    "y = west_nile['WnvPresent']\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, stratify = y)\n",
    "\n",
    "# Oversample minority class in y\n",
    "smote = SMOTE()\n",
    "X_train, y_train = smote.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions: \n",
    "\n",
    "The below set-up can be run to fit a Random Forest Classifier after gridsearching over the hyperparameters 'max depth' and 'max features'. The set-up can take in the output from any of the three cells above, just be aware of which matrix you are using and edit the 'Model' argument in the get_classification_results() function if you are trying to save down the results to ensure appropriate labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForest Model\n",
    "\n",
    "# Set up pipeline\n",
    "steps = [\n",
    "    ('poly', PolynomialFeatures()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "]\n",
    "\n",
    "pipe = Pipeline(steps)\n",
    "\n",
    "# Set up gridsearch\n",
    "grid_params = {\n",
    "    'poly__degree':[1,2,3],\n",
    "    'poly__interaction_only':[True, False],\n",
    "    'rf__max_depth':[None, 1, 3, 5 ,7],\n",
    "    'rf__max_features':[None, 'auto', 2, 3, 4, 5]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "gs = GridSearchCV(pipe, grid_params, scoring = 'roc_auc', n_jobs = -1)\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "# Run and evaluate model\n",
    "model_dict, cm = get_classification_results(gs, 'Random Forest (basic)',\n",
    "                                            X_train, X_test,\n",
    "                                            y_train, y_test)\n",
    "\n",
    "ensemble_models.append(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best parameters from the grid search\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the model results as a dataframe\n",
    "edf = pd.DataFrame(ensemble_models)\n",
    "edf = edf[['Model','Train Accuracy','Test Accuracy','Precision','Recall','Support','F_1 Score','ROC-AUC']]\n",
    "edf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on accuracy alone, this is a slight improvement on the decision tree approach. However, these are still not satisfactory results. Given more time, I still think that finding new data and engineering better features is the best path forward, and figuring out how to get more variance in the models within the ensemble may correct for the overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
